{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.454980</td>\n",
       "      <td>534.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.648037</td>\n",
       "      <td>8854.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>1.120856</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.264763</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id proto state       dur  sbytes  dbytes   sttl   dttl  sloss  dloss  ...  \\\n",
       "0   0   tcp   FIN  0.454980   534.0   268.0  254.0  252.0    2.0    1.0  ...   \n",
       "1   1   tcp   FIN  0.648037  8854.0   268.0  254.0  252.0    4.0    1.0  ...   \n",
       "2   2   tcp   FIN  1.120856  3440.0   642.0  254.0  252.0    5.0    3.0  ...   \n",
       "3   3   udp   INT  0.000001   244.0     0.0  254.0    NaN    0.0    0.0  ...   \n",
       "4   4   tcp   FIN  0.264763  1540.0  1644.0   31.0   29.0    4.0    4.0  ...   \n",
       "\n",
       "  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
       "0              0.0           0.0         0.0         5.0         5.0   \n",
       "1              0.0           NaN         0.0         6.0         6.0   \n",
       "2              0.0           0.0         0.0         4.0         4.0   \n",
       "3              0.0           0.0         0.0        10.0         4.0   \n",
       "4              NaN           0.0         0.0        13.0        11.0   \n",
       "\n",
       "   ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
       "0         2.0         2.0               2.0               1.0             2.0  \n",
       "1         1.0         1.0               1.0               1.0             5.0  \n",
       "2         1.0         2.0               1.0               1.0             4.0  \n",
       "3         2.0         4.0               2.0               1.0             4.0  \n",
       "4        10.0         7.0               6.0               1.0             7.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../dataset_train.csv')\n",
    "df = df.drop(columns=[\"label\"])\n",
    "df_test = pd.read_csv('../../dataset_test.csv')\n",
    "df.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['proto', 'state', 'service','is_sm_ips_ports','is_ftp_login','attack_cat']\n",
    "noncategorical_features = [col for col in df.columns.tolist() if col not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = df.copy()\n",
    "le_attack_cat = LabelEncoder()\n",
    "df['attack_cat'] = le_attack_cat.fit_transform(df['attack_cat'])\n",
    "\n",
    "train_set, val_set = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class FeatureImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='mean', fill_value=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputer for handling missing values.\n",
    "\n",
    "        :param strategy: The strategy to use for imputation ('mean', 'median', 'most_frequent', 'constant').\n",
    "                         Default is 'mean'.\n",
    "        :param fill_value: The value to use for the 'constant' strategy. Default is None.\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.imputer = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the imputer to the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        \"\"\"\n",
    "        self.imputer.fit(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data by imputing the missing values.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer and transform the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.fit_transform(X)\n",
    "\n",
    "    def get_imputation_statistics(self):\n",
    "        \"\"\"\n",
    "        Get the imputation statistics (e.g., mean or median values used for imputation).\n",
    "\n",
    "        :return: The statistics used for imputation (depending on the strategy)\n",
    "        \"\"\"\n",
    "        return self.imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class FeatureImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='mean', fill_value=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputer for handling missing values.\n",
    "\n",
    "        :param strategy: The strategy to use for imputation ('mean', 'median', 'most_frequent', 'constant').\n",
    "                         Default is 'mean'.\n",
    "        :param fill_value: The value to use for the 'constant' strategy. Default is None.\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.imputer = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the imputer to the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        \"\"\"\n",
    "        self.imputer.fit(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data by imputing the missing values.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer and transform the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.fit_transform(X)\n",
    "\n",
    "    def get_imputation_statistics(self):\n",
    "        \"\"\"\n",
    "        Get the imputation statistics (e.g., mean or median values used for imputation).\n",
    "\n",
    "        :return: The statistics used for imputation (depending on the strategy)\n",
    "        \"\"\"\n",
    "        return self.imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame during fitting\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        self.lower_bounds = np.percentile(X, self.lower_percentile * 100, axis=0)\n",
    "        self.upper_bounds = np.percentile(X, self.upper_percentile * 100, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure X is a NumPy array during transformation\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        return np.clip(X, self.lower_bounds, self.upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateRemover(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X)\n",
    "        X_unique = X_df.drop_duplicates()\n",
    "        if y is not None:\n",
    "            y_unique = y[X_df.index.isin(X_unique.index)]\n",
    "        return X_unique.values, y_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "class FeatureSelection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=10, score_func=f_classif):\n",
    "        \"\"\"\n",
    "        Initialize the feature selection process.\n",
    "\n",
    "        :param k: Number of top features to select. Default is 10.\n",
    "        :param score_func: Scoring function to evaluate the features. Default is f_classif (ANOVA F-test).\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.score_func = score_func\n",
    "        self.selector = SelectKBest(score_func=self.score_func, k=self.k)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the selector to the data.\n",
    "\n",
    "        :param X: Features\n",
    "        :param y: Target labels\n",
    "        \"\"\"\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        self.selector.fit(X, y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply the feature selection transformation.\n",
    "\n",
    "        :param X: Features to transform\n",
    "        :return: Transformed features\n",
    "        \"\"\"\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def fit_transform(self, X,y):\n",
    "        \"\"\"\n",
    "        Fit the selector and apply the transformation.\n",
    "\n",
    "        :param X: Features\n",
    "        :param y: Target labels\n",
    "        :return: Transformed features\n",
    "        \"\"\"\n",
    "        return self.selector.fit_transform(X, y)\n",
    "\n",
    "    def get_support(self):\n",
    "        \"\"\"\n",
    "        Get the mask of selected features.\n",
    "\n",
    "        :return: Mask of selected features (True/False)\n",
    "        \"\"\"\n",
    "        return self.selector.get_support()\n",
    "\n",
    "    def get_selected_features(self):\n",
    "        \"\"\"\n",
    "        Get the indices of the selected features.\n",
    "\n",
    "        :return: List of selected feature indices\n",
    "        \"\"\"\n",
    "        return self.selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method=\"standard\"):\n",
    "        self.method = method\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == \"standard\":\n",
    "            self.scaler = StandardScaler().fit(X)\n",
    "        elif self.method == \"minmax\":\n",
    "            self.scaler = MinMaxScaler().fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X) if self.scaler else X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEncodingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, onehot_columns=None, label_columns=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - onehot_columns: List of column indices for one-hot encoding.\n",
    "        - label_columns: List of column indices for label encoding.\n",
    "        \"\"\"\n",
    "        self.onehot_columns = onehot_columns or []\n",
    "        self.label_columns = label_columns or []\n",
    "        self.onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') if self.onehot_columns else None\n",
    "        self.label_encoder = LabelEncoder() if self.label_columns else None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformers to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        - y: Optional target labels, not used in this transformer.\n",
    "        \"\"\"\n",
    "        if self.onehot_columns:\n",
    "            # Fit one-hot encoder for the specified columns\n",
    "            self.onehot_encoder.fit(X[:, self.onehot_columns])\n",
    "\n",
    "        if self.label_columns:\n",
    "            # Fit label encoder for the specified columns\n",
    "            for col in self.label_columns:\n",
    "                self.label_encoder.fit(X[:, col])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data using the appropriate encoding methods.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        if self.onehot_columns:\n",
    "            onehot_encoded = self.onehot_encoder.transform(X[:, self.onehot_columns])\n",
    "            # Replace the original columns with one-hot encoded columns\n",
    "            X_transformed = np.delete(X_transformed, self.onehot_columns, axis=1)\n",
    "            X_transformed = np.hstack([X_transformed, onehot_encoded])\n",
    "\n",
    "        if self.label_columns:\n",
    "            for col in self.label_columns:\n",
    "                X_transformed[:, col] = self.label_encoder.transform(X[:, col])\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformers and transform the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        - y: Optional target labels, not used in this transformer.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, random_state=None, sampling_strategy='auto'):\n",
    "        \"\"\"\n",
    "        Initialize the SMOTE handler.\n",
    "\n",
    "        :param random_state: Random state for reproducibility (default is None)\n",
    "        :param sampling_strategy: Defines the sampling strategy for SMOTE (default is 'auto')\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.smote = SMOTE(random_state=self.random_state, sampling_strategy=self.sampling_strategy)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SMOTE model to the training data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.smote.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Perform the oversampling on the training data and return the balanced dataset.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector (optional, but required if using `fit`)\n",
    "        :return: Balanced feature matrix X, and target vector y\n",
    "        \"\"\"\n",
    "        if y is None:\n",
    "            raise ValueError(\"Target vector y must be provided for transforming the dataset\")\n",
    "        X_resampled, y_resampled = self.smote.fit_resample(X, y)\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit and transform the dataset in one step.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :return: Balanced feature matrix X, and target vector y\n",
    "        \"\"\"\n",
    "        return self.smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "class DataNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, norm='l2'):\n",
    "        \"\"\"\n",
    "        Initialize the data normalizer.\n",
    "\n",
    "        :param norm: Norm to use for normalization, can be 'l1', 'l2', or 'max'. Default is 'l2'.\n",
    "        \"\"\"\n",
    "        self.norm = norm\n",
    "        self.normalizer = Normalizer(norm=self.norm)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the normalizer to the data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector (optional)\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.normalizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Normalize the data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :return: Normalized feature matrix\n",
    "        \"\"\"\n",
    "        return self.normalizer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform the data in one step.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector (optional)\n",
    "        :return: Normalized feature matrix\n",
    "        \"\"\"\n",
    "        return self.normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReducer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=None):\n",
    "        \"\"\"\n",
    "        Initialize the PCA dimensionality reducer.\n",
    "\n",
    "        :param n_components: Number of principal components to keep.\n",
    "                              If None, keeps all components.\n",
    "                              Can also be a float (explained variance ratio).\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=self.n_components)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the PCA model to the data.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector (optional).\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.pca.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data to the lower-dimensional space.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :return: Transformed data in lower-dimensional space.\n",
    "        \"\"\"\n",
    "        return self.pca.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform the data in one step.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector (optional).\n",
    "        :return: Transformed data in lower-dimensional space.\n",
    "        \"\"\"\n",
    "        return self.pca.fit_transform(X)\n",
    "\n",
    "    def explained_variance_ratio(self):\n",
    "        \"\"\"\n",
    "        Return the explained variance ratio of each principal component.\n",
    "\n",
    "        :return: Array of explained variance ratios for each component.\n",
    "        \"\"\"\n",
    "        return self.pca.explained_variance_ratio_\n",
    "\n",
    "    def components(self):\n",
    "        \"\"\"\n",
    "        Return the principal components (eigenvectors).\n",
    "\n",
    "        :return: Matrix of principal components.\n",
    "        \"\"\"\n",
    "        return self.pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDiscretizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features, bins=10, strategy='uniform'):\n",
    "        self.features = features  # List of features to discretize\n",
    "        self.bins = bins  # Number of bins or discretization strategy\n",
    "        self.strategy = strategy  # Discretization strategy (e.g., 'uniform', 'quantile')\n",
    "        self.discretizers = {}  # Store discretizers for each feature\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X, columns=self.features)  # Create DataFrame for easier handling\n",
    "\n",
    "        for feature in self.features:\n",
    "            # Create discretizer based on the chosen strategy\n",
    "            if self.strategy == 'uniform':\n",
    "                discretizer = np.linspace(X_df[feature].min(), X_df[feature].max(), self.bins + 1)\n",
    "            elif self.strategy == 'quantile':\n",
    "                discretizer = np.quantile(X_df[feature], np.linspace(0, 1, self.bins + 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid strategy: {self.strategy}\")\n",
    "\n",
    "            self.discretizers[feature] = discretizer\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X, columns=self.features)  # Create DataFrame for easier handling\n",
    "\n",
    "        for feature in self.features:\n",
    "            discretizer = self.discretizers[feature]\n",
    "            # Apply discretization to the feature\n",
    "            X_df[feature] = pd.cut(X_df[feature], bins=discretizer, labels=False, include_lowest=True, duplicates='drop')\n",
    "\n",
    "        return X_df.values # Convert back to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_without_target = [x for x in categorical_features if x != 'attack_cat']\n",
    "\n",
    "# Updated numeric transformer pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='median')),\n",
    "    ('outlier_clipper', OutlierClipper(lower_percentile=0.01, upper_percentile=0.99)),\n",
    "    ('discretizer', FeatureDiscretizer(features=noncategorical_features, bins=10, strategy='uniform')),\n",
    "    ('scaler', FeatureScaling(method='standard'))\n",
    "])\n",
    "\n",
    "# Updated categorical transformer pipeline using LabelEncoder\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Define preprocessor with ColumnTransformer using your custom classes\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, noncategorical_features),\n",
    "        ('cat', categorical_transformer, categorical_without_target) # Using categorical_without_target\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Final pipeline using your custom classes\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    # ('smote', SMOTEHandler(random_state=42, sampling_strategy='auto')),\n",
    "    # ('dimensionality_reducer', DimensionalityReducer(n_components=5)),\n",
    "    # ('feature_selector', FeatureSelection(k=10)),\n",
    "    # ('duplicate_remover', DuplicateRemover()),\n",
    "    # ('scaler', DataNormalizer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56634064\n",
      "Precision: 0.86825840\n",
      "Recall: 0.56634064\n",
      "\n",
      "Sklearn DecisionTreeClassifier Evaluation:\n",
      "Accuracy: 0.69123728\n",
      "Precision: 0.67871074\n",
      "Recall: 0.69123728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Initialize the ID3 Decision Tree.\n",
    "\n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the tree (default: None, meaning no limit).\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of a target variable.\n",
    "\n",
    "        Parameters:\n",
    "        - y: Target vector.\n",
    "        \"\"\"\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def information_gain(self, X_column, y):\n",
    "        \"\"\"\n",
    "        Calculate information gain of a split on a feature.\n",
    "\n",
    "        Parameters:\n",
    "        - X_column: Feature values.\n",
    "        - y: Target vector.\n",
    "        \"\"\"\n",
    "        parent_entropy = self.entropy(y)\n",
    "\n",
    "        # Split the data based on unique feature values\n",
    "        values, counts = np.unique(X_column, return_counts=True)\n",
    "        weighted_entropy = np.sum(\n",
    "            [(counts[i] / len(X_column)) * self.entropy(y[X_column == value])\n",
    "             for i, value in enumerate(values)]\n",
    "        )\n",
    "        return parent_entropy - weighted_entropy\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split on.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix.\n",
    "        - y: Target vector.\n",
    "        \"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            gain = self.information_gain(X[:, feature], y)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "\n",
    "        return best_feature\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Build the decision tree recursively.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix.\n",
    "        - y: Target vector.\n",
    "        - depth: Current depth of the tree.\n",
    "        \"\"\"\n",
    "        # Stop conditions\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return y[0]  # Leaf node with class label\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return Counter(y).most_common(1)[0][0]  # Majority class\n",
    "        if X.shape[1] == 0:\n",
    "            return Counter(y).most_common(1)[0][0]  # Majority class\n",
    "\n",
    "        # Find the best feature to split\n",
    "        feature = self.best_split(X, y)\n",
    "        if feature is None:\n",
    "            return Counter(y).most_common(1)[0][0]  # Majority class\n",
    "\n",
    "        tree = {feature: {}}\n",
    "        for value in np.unique(X[:, feature]):\n",
    "            sub_X = X[X[:, feature] == value]\n",
    "            sub_y = y[X[:, feature] == value]\n",
    "            subtree = self.build_tree(\n",
    "                np.delete(sub_X, feature, axis=1), sub_y, depth + 1\n",
    "            )\n",
    "            tree[feature][value] = subtree\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree to the training data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix.\n",
    "        - y: Target vector.\n",
    "        \"\"\"\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, tree, sample):\n",
    "        \"\"\"\n",
    "        Predict the class label for a single sample using the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        - tree: Decision tree.\n",
    "        - sample: Feature vector of the sample.\n",
    "        \"\"\"\n",
    "        # If the current node is a leaf, return its value\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        # Get the feature index for the current node\n",
    "        feature = next(iter(tree))\n",
    "        value = sample[feature]\n",
    "\n",
    "        # If the value is not present in the subtree, return the majority class of the subtree\n",
    "        if value not in tree[feature]:\n",
    "            # Calculate the majority class of the subtree\n",
    "            subtree = tree[feature]\n",
    "            majority_class = Counter(\n",
    "                leaf for sub in subtree.values() for leaf in self.get_all_leaves(sub)\n",
    "            ).most_common(1)[0][0]\n",
    "            return majority_class\n",
    "\n",
    "        # Recurse down the tree\n",
    "        subtree = tree[feature][value]\n",
    "        return self.predict_sample(subtree, sample)\n",
    "\n",
    "    def get_all_leaves(self, tree):\n",
    "        \"\"\"\n",
    "        Helper function to collect all leaf values from a subtree.\n",
    "\n",
    "        Parameters:\n",
    "        - tree: Decision tree or subtree.\n",
    "\n",
    "        Returns:\n",
    "        - List of leaf values.\n",
    "        \"\"\"\n",
    "        if not isinstance(tree, dict):\n",
    "            return [tree]\n",
    "        leaves = []\n",
    "        for subtree in tree.values():\n",
    "            leaves.extend(self.get_all_leaves(subtree))\n",
    "        return leaves\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for a dataset.\n",
    "        Parameters:\n",
    "        - X: Feature matrix.\n",
    "        \"\"\"\n",
    "        return np.array([self.predict_sample(self.tree, sample) for sample in X])\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Evaluate the model using accuracy, recall, and precision.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Feature matrix.\n",
    "        - y_true: True class labels.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.8f}\")\n",
    "        print(f\"Precision: {precision:.8f}\")\n",
    "        print(f\"Recall: {recall:.8f}\")\n",
    "\n",
    "# Example usage:\n",
    "x_train_set = train_set.drop('attack_cat', axis=1)\n",
    "y_train_set = train_set['attack_cat']\n",
    "x_val_set = val_set.drop('attack_cat', axis=1)\n",
    "y_val_set = val_set['attack_cat']\n",
    "\n",
    "x_train_set_processed = pipe.fit_transform(x_train_set, y_train_set)\n",
    "x_val_set_processed = pipe.transform(x_val_set)\n",
    "\n",
    "id3 = ID3DecisionTree(max_depth=5)\n",
    "\n",
    "# Train the model\n",
    "id3.fit(x_train_set_processed, y_train_set.values)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "id3.evaluate(x_val_set_processed, y_val_set.values)\n",
    "\n",
    "sklearn_tree = DecisionTreeClassifier(max_depth=5, criterion='entropy', random_state=42)\n",
    "\n",
    "# Train the sklearn DecisionTree model\n",
    "sklearn_tree.fit(x_train_set_processed, y_train_set.values)\n",
    "\n",
    "# Predict using the sklearn DecisionTree model\n",
    "val_predictions_sklearn = sklearn_tree.predict(x_val_set_processed)\n",
    "\n",
    "# Evaluate the sklearn DecisionTree model\n",
    "accuracy = accuracy_score(y_val_set, val_predictions_sklearn)\n",
    "precision = precision_score(y_val_set, val_predictions_sklearn, average='weighted')\n",
    "recall = recall_score(y_val_set, val_predictions_sklearn, average='weighted')\n",
    "\n",
    "print(\"\\nSklearn DecisionTreeClassifier Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy:.8f}\")\n",
    "print(f\"Precision: {precision:.8f}\")\n",
    "print(f\"Recall: {recall:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../model-id3.pkl', 'wb') as file:\n",
    "    pickle.dump(id3, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../model-id3.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_set = df.copy()\n",
    "x_training_set = training_set.drop('attack_cat', axis=1)\n",
    "y_training_set = training_set['attack_cat']\n",
    "\n",
    "x_test_set = df_test.copy()\n",
    "\n",
    "x_training_set_processed = pipe.fit_transform(x_training_set, y_training_set)\n",
    "x_test_set_processed = pipe.transform(x_test_set)\n",
    "\n",
    "loaded_model.fit(x_training_set_processed, y_training_set.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = loaded_model.predict(x_test_set_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 ... 3 3 3]\n",
      "['Exploits' 'Exploits' 'Exploits' ... 'Exploits' 'Exploits' 'Exploits']\n"
     ]
    }
   ],
   "source": [
    "print(y_test_predict)\n",
    "reversed = le_attack_cat.inverse_transform(y_test_predict)\n",
    "print(reversed)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    \"id\": range(len(reversed)),\n",
    "    \"attack_cat\": reversed\n",
    "})\n",
    "\n",
    "result_df.head()\n",
    "result_df.to_csv(\"predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
