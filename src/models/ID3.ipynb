{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.454980</td>\n",
       "      <td>534.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.648037</td>\n",
       "      <td>8854.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>1.120856</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.264763</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id proto state       dur  sbytes  dbytes   sttl   dttl  sloss  dloss  ...  \\\n",
       "0   0   tcp   FIN  0.454980   534.0   268.0  254.0  252.0    2.0    1.0  ...   \n",
       "1   1   tcp   FIN  0.648037  8854.0   268.0  254.0  252.0    4.0    1.0  ...   \n",
       "2   2   tcp   FIN  1.120856  3440.0   642.0  254.0  252.0    5.0    3.0  ...   \n",
       "3   3   udp   INT  0.000001   244.0     0.0  254.0    NaN    0.0    0.0  ...   \n",
       "4   4   tcp   FIN  0.264763  1540.0  1644.0   31.0   29.0    4.0    4.0  ...   \n",
       "\n",
       "  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
       "0              0.0           0.0         0.0         5.0         5.0   \n",
       "1              0.0           NaN         0.0         6.0         6.0   \n",
       "2              0.0           0.0         0.0         4.0         4.0   \n",
       "3              0.0           0.0         0.0        10.0         4.0   \n",
       "4              NaN           0.0         0.0        13.0        11.0   \n",
       "\n",
       "   ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
       "0         2.0         2.0               2.0               1.0             2.0  \n",
       "1         1.0         1.0               1.0               1.0             5.0  \n",
       "2         1.0         2.0               1.0               1.0             4.0  \n",
       "3         2.0         4.0               2.0               1.0             4.0  \n",
       "4        10.0         7.0               6.0               1.0             7.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../dataset_train.csv')\n",
    "df = df.drop(columns=[\"label\"])\n",
    "df_test = pd.read_csv('../../dataset_test.csv')\n",
    "df.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['proto', 'state', 'service','is_sm_ips_ports','is_ftp_login','attack_cat']\n",
    "noncategorical_features = [col for col in df.columns.tolist() if col not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = df.copy()\n",
    "le_attack_cat = LabelEncoder()\n",
    "df['attack_cat'] = le_attack_cat.fit_transform(df['attack_cat'])\n",
    "\n",
    "train_set, val_set = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class FeatureImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='mean', fill_value=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputer for handling missing values.\n",
    "\n",
    "        :param strategy: The strategy to use for imputation ('mean', 'median', 'most_frequent', 'constant').\n",
    "                         Default is 'mean'.\n",
    "        :param fill_value: The value to use for the 'constant' strategy. Default is None.\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.imputer = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the imputer to the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        \"\"\"\n",
    "        self.imputer.fit(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data by imputing the missing values.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer and transform the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.fit_transform(X)\n",
    "\n",
    "    def get_imputation_statistics(self):\n",
    "        \"\"\"\n",
    "        Get the imputation statistics (e.g., mean or median values used for imputation).\n",
    "\n",
    "        :return: The statistics used for imputation (depending on the strategy)\n",
    "        \"\"\"\n",
    "        return self.imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame during fitting\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        self.lower_bounds = np.percentile(X, self.lower_percentile * 100, axis=0)\n",
    "        self.upper_bounds = np.percentile(X, self.upper_percentile * 100, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure X is a NumPy array during transformation\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        return np.clip(X, self.lower_bounds, self.upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateRemover(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X,y)\n",
    "        X_unique, indices = np.unique(X[0], axis=0, return_index=True)\n",
    "        y_unique = X[1][indices]\n",
    "        return X_unique, y_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "class FeatureSelection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=10, score_func=f_classif):\n",
    "        \"\"\"\n",
    "        Initialize the feature selection process.\n",
    "\n",
    "        :param k: Number of top features to select. Default is 10.\n",
    "        :param score_func: Scoring function to evaluate the features. Default is f_classif (ANOVA F-test).\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.score_func = score_func\n",
    "        self.selector = SelectKBest(score_func=self.score_func, k=self.k)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the selector to the data.\n",
    "\n",
    "        :param X: Features\n",
    "        :param y: Target labels\n",
    "        \"\"\"\n",
    "        self.selector.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Apply the feature selection transformation.\n",
    "\n",
    "        :param X: Features to transform\n",
    "        :return: Transformed features\n",
    "        \"\"\"\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def fit_transform(self, X,y):\n",
    "        \"\"\"\n",
    "        Fit the selector and apply the transformation.\n",
    "\n",
    "        :param X: Features\n",
    "        :param y: Target labels\n",
    "        :return: Transformed features\n",
    "        \"\"\"\n",
    "        return self.selector.fit_transform(X, y)\n",
    "\n",
    "    def get_support(self):\n",
    "        \"\"\"\n",
    "        Get the mask of selected features.\n",
    "\n",
    "        :return: Mask of selected features (True/False)\n",
    "        \"\"\"\n",
    "        return self.selector.get_support()\n",
    "\n",
    "    def get_selected_features(self):\n",
    "        \"\"\"\n",
    "        Get the indices of the selected features.\n",
    "\n",
    "        :return: List of selected feature indices\n",
    "        \"\"\"\n",
    "        return self.selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method=\"standard\"):\n",
    "        self.method = method\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == \"standard\":\n",
    "            self.scaler = StandardScaler().fit(X)\n",
    "        elif self.method == \"minmax\":\n",
    "            self.scaler = MinMaxScaler().fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X) if self.scaler else X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEncodingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, onehot_columns=None, label_columns=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - onehot_columns: List of column indices for one-hot encoding.\n",
    "        - label_columns: List of column indices for label encoding.\n",
    "        \"\"\"\n",
    "        self.onehot_columns = onehot_columns or []\n",
    "        self.label_columns = label_columns or []\n",
    "        self.onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') if self.onehot_columns else None\n",
    "        self.label_encoder = LabelEncoder() if self.label_columns else None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformers to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        - y: Optional target labels, not used in this transformer.\n",
    "        \"\"\"\n",
    "        if self.onehot_columns:\n",
    "            # Fit one-hot encoder for the specified columns\n",
    "            self.onehot_encoder.fit(X[:, self.onehot_columns])\n",
    "\n",
    "        if self.label_columns:\n",
    "            # Fit label encoder for the specified columns\n",
    "            for col in self.label_columns:\n",
    "                self.label_encoder.fit(X[:, col])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data using the appropriate encoding methods.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        if self.onehot_columns:\n",
    "            onehot_encoded = self.onehot_encoder.transform(X[:, self.onehot_columns])\n",
    "            # Replace the original columns with one-hot encoded columns\n",
    "            X_transformed = np.delete(X_transformed, self.onehot_columns, axis=1)\n",
    "            X_transformed = np.hstack([X_transformed, onehot_encoded])\n",
    "\n",
    "        if self.label_columns:\n",
    "            for col in self.label_columns:\n",
    "                X_transformed[:, col] = self.label_encoder.transform(X[:, col])\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformers and transform the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        - y: Optional target labels, not used in this transformer.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, random_state=None, sampling_strategy='auto'):\n",
    "        \"\"\"\n",
    "        Initialize the SMOTE handler.\n",
    "\n",
    "        :param random_state: Random state for reproducibility (default is None)\n",
    "        :param sampling_strategy: Defines the sampling strategy for SMOTE (default is 'auto')\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.smote = SMOTE(random_state=self.random_state, sampling_strategy=self.sampling_strategy)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SMOTE model to the training data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.smote.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit and transform the dataset in one step.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :return: Balanced feature matrix X, and target vector y\n",
    "        \"\"\"\n",
    "        return self.smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "class DataNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, norm='l2'):\n",
    "        \"\"\"\n",
    "        Initialize the data normalizer.\n",
    "\n",
    "        :param norm: Norm to use for normalization, can be 'l1', 'l2', or 'max'. Default is 'l2'.\n",
    "        \"\"\"\n",
    "        self.norm = norm\n",
    "        self.normalizer = Normalizer(norm=self.norm)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the normalizer to the data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector (optional)\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.normalizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Normalize the data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :return: Normalized feature matrix\n",
    "        \"\"\"\n",
    "        return self.normalizer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform the data in one step.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector (optional)\n",
    "        :return: Normalized feature matrix\n",
    "        \"\"\"\n",
    "        return self.normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReducer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=None):\n",
    "        \"\"\"\n",
    "        Initialize the PCA dimensionality reducer.\n",
    "\n",
    "        :param n_components: Number of principal components to keep.\n",
    "                              If None, keeps all components.\n",
    "                              Can also be a float (explained variance ratio).\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=self.n_components)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the PCA model to the data.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector (optional).\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.pca.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data to the lower-dimensional space.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :return: Transformed data in lower-dimensional space.\n",
    "        \"\"\"\n",
    "        return self.pca.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform the data in one step.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector (optional).\n",
    "        :return: Transformed data in lower-dimensional space.\n",
    "        \"\"\"\n",
    "        return self.pca.fit_transform(X[0])\n",
    "\n",
    "    def explained_variance_ratio(self):\n",
    "        \"\"\"\n",
    "        Return the explained variance ratio of each principal component.\n",
    "\n",
    "        :return: Array of explained variance ratios for each component.\n",
    "        \"\"\"\n",
    "        return self.pca.explained_variance_ratio_\n",
    "\n",
    "    def components(self):\n",
    "        \"\"\"\n",
    "        Return the principal components (eigenvectors).\n",
    "\n",
    "        :return: Matrix of principal components.\n",
    "        \"\"\"\n",
    "        return self.pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDiscretizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features, bins=10, strategy='uniform'):\n",
    "        self.features = features  # List of features to discretize\n",
    "        self.bins = bins  # Number of bins or discretization strategy\n",
    "        self.strategy = strategy  # Discretization strategy (e.g., 'uniform', 'quantile')\n",
    "        self.discretizers = {}  # Store discretizers for each feature\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X, columns=self.features)  # Create DataFrame for easier handling\n",
    "\n",
    "        for feature in self.features:\n",
    "            # Create discretizer based on the chosen strategy\n",
    "            if self.strategy == 'uniform':\n",
    "                discretizer = np.linspace(X_df[feature].min(), X_df[feature].max(), self.bins + 1)\n",
    "            elif self.strategy == 'quantile':\n",
    "                discretizer = np.quantile(X_df[feature], np.linspace(0, 1, self.bins + 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid strategy: {self.strategy}\")\n",
    "\n",
    "            self.discretizers[feature] = discretizer\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X, columns=self.features)  # Create DataFrame for easier handling\n",
    "\n",
    "        for feature in self.features:\n",
    "            discretizer = self.discretizers[feature]\n",
    "            # Apply discretization to the feature\n",
    "            X_df[feature] = pd.cut(X_df[feature], bins=discretizer, labels=False, include_lowest=True, duplicates='drop')\n",
    "\n",
    "        return X_df.values # Convert back to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "categorical_without_target = [x for x in categorical_features if x != 'attack_cat']\n",
    "onehot_features = ['service', 'proto']\n",
    "label_features = ['state']\n",
    "\n",
    "numeric_transformer_id3 = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='median')),\n",
    "    ('outlier_clipper', OutlierClipper(lower_percentile=0.01, upper_percentile=0.99)),\n",
    "    ('discretizer', FeatureDiscretizer(features=noncategorical_features, bins=10, strategy='uniform')),\n",
    "    ('scaler', FeatureScaling(method='standard'))\n",
    "])\n",
    "\n",
    "categorical_transformer_id3 = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "preprocessor_id3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_id3, noncategorical_features),\n",
    "        ('cat', categorical_transformer_id3, categorical_without_target)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_id3 = ImbPipeline([\n",
    "    ('preprocessor', preprocessor_id3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train_set_id3 = train_set.drop('attack_cat', axis=1)\n",
    "y_train_set_id3 = train_set['attack_cat']\n",
    "x_val_set_id3 = val_set.drop('attack_cat', axis=1)\n",
    "y_val_set_id3 = val_set['attack_cat']\n",
    "x_train_set_processed_id3 = pipe_id3.fit_transform(x_train_set_id3, y_train_set_id3)\n",
    "\n",
    "x_val_set_processed_id3 = pipe_id3.transform(x_val_set_id3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7354\n",
      "Confusion Matrix:\n",
      "[[  49   18   99  165   12    3   27   10    0    1]\n",
      " [  19   21  100  176   23    2    8   14    4    0]\n",
      " [ 112   98  775 1210   87   29   50   80   16    2]\n",
      " [ 150  143 1107 4330  298   77  196  303   18   14]\n",
      " [  19   40  144  431 1744   20  824  361   53    1]\n",
      " [   3    1   43   82   26 7803    7   10    0    0]\n",
      " [  31    5   57  196  923   17 9840  158   27    0]\n",
      " [   5   20  127  372  239    7   98 1190   10    2]\n",
      " [   0    3   13   23   68    1   34   81   36    0]\n",
      " [   0    0    1   13    2    0    1    8    1    2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.13      0.13       384\n",
      "           1       0.06      0.06      0.06       367\n",
      "           2       0.31      0.32      0.31      2459\n",
      "           3       0.62      0.65      0.64      6636\n",
      "           4       0.51      0.48      0.49      3637\n",
      "           5       0.98      0.98      0.98      7975\n",
      "           6       0.89      0.87      0.88     11254\n",
      "           7       0.54      0.57      0.56      2070\n",
      "           8       0.22      0.14      0.17       259\n",
      "           9       0.09      0.07      0.08        28\n",
      "\n",
      "    accuracy                           0.74     35069\n",
      "   macro avg       0.43      0.43      0.43     35069\n",
      "weighted avg       0.74      0.74      0.74     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Inisialisasi DecisionTreeClassifier dengan kriteria 'entropy'\n",
    "id3 = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "# Fit model ke data latih\n",
    "id3.fit(x_train_set_processed_id3, y_train_set_id3)\n",
    "\n",
    "# Prediksi data validasi\n",
    "y_pred = id3.predict(x_val_set_processed_id3)\n",
    "\n",
    "# Hitung akurasi\n",
    "accuracy = accuracy_score(y_val_set_id3, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Matriks kebingungan (confusion matrix)\n",
    "conf_matrix = confusion_matrix(y_val_set_id3, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Laporan klasifikasi\n",
    "class_report = classification_report(y_val_set_id3, y_pred, zero_division=0)\n",
    "print(f\"Classification Report:\\n{class_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7346\n",
      "Confusion Matrix:\n",
      "[[  27    7   72  257    6    1   10    4    0    0]\n",
      " [   6    5   65  226   28    1   23   13    0    0]\n",
      " [  21   35  431 1644  118   17  113   76    4    0]\n",
      " [  45   41  575 5101  340   23  360  146    5    0]\n",
      " [  10    8   57  384 1932   22  638  572   14    0]\n",
      " [   1    0    8  102   40 7764   17   43    0    0]\n",
      " [   3    0    0  269 1175   11 9381  412    3    0]\n",
      " [   4    6   62  527  296    2   75 1098    0    0]\n",
      " [   0    0    0    0   78    0   70   90   21    0]\n",
      " [   0    0    0   24    2    0    1    1    0    0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.07      0.11       384\n",
      "           1       0.05      0.01      0.02       367\n",
      "           2       0.34      0.18      0.23      2459\n",
      "           3       0.60      0.77      0.67      6636\n",
      "           4       0.48      0.53      0.50      3637\n",
      "           5       0.99      0.97      0.98      7975\n",
      "           6       0.88      0.83      0.86     11254\n",
      "           7       0.45      0.53      0.49      2070\n",
      "           8       0.45      0.08      0.14       259\n",
      "           9       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.73     35069\n",
      "   macro avg       0.45      0.40      0.40     35069\n",
      "weighted avg       0.73      0.73      0.72     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_gain=1e-4, use_gini=False):\n",
    "        \"\"\"\n",
    "        Initialize the ID3 Decision Tree.\n",
    "\n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the tree (default: None, meaning no limit).\n",
    "        - min_samples_split: Minimum samples required to split a node.\n",
    "        - min_gain: Minimum information gain required for a split.\n",
    "        - use_gini: Use Gini Impurity instead of Entropy.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_gain = min_gain\n",
    "        self.use_gini = use_gini\n",
    "        self.tree = None\n",
    "\n",
    "    def entropy(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def gini(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return 1 - np.sum([p ** 2 for p in probabilities])\n",
    "\n",
    "    def impurity(self, y):\n",
    "        return self.gini(y) if self.use_gini else self.entropy(y)\n",
    "\n",
    "    def information_gain(self, X_column, y):\n",
    "        parent_impurity = self.impurity(y)\n",
    "        values, counts = np.unique(X_column, return_counts=True)\n",
    "\n",
    "        weighted_impurity = np.sum(\n",
    "            [(counts[i] / len(X_column)) * self.impurity(y[X_column == value])\n",
    "             for i, value in enumerate(values)]\n",
    "        )\n",
    "        return parent_impurity - weighted_impurity\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            gain = self.information_gain(X[:, feature], y)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "\n",
    "        if best_gain < self.min_gain:\n",
    "            return None\n",
    "        return best_feature\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return y[0]\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        feature = self.best_split(X, y)\n",
    "        if feature is None:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        tree = {feature: {}}\n",
    "        for value in np.unique(X[:, feature]):\n",
    "            sub_X = X[X[:, feature] == value]\n",
    "            sub_y = y[X[:, feature] == value]\n",
    "            subtree = self.build_tree(sub_X, sub_y, depth + 1)\n",
    "            tree[feature][value] = subtree\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, tree, sample):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        feature = next(iter(tree))\n",
    "        value = sample[feature]\n",
    "        subtree = tree[feature].get(value)\n",
    "        if subtree is None:\n",
    "            return Counter(self.get_all_leaves(tree)).most_common(1)[0][0]\n",
    "        return self.predict_sample(subtree, sample)\n",
    "\n",
    "    def get_all_leaves(self, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return [tree]\n",
    "        leaves = []\n",
    "        for subtree in tree.values():\n",
    "            leaves.extend(self.get_all_leaves(subtree))\n",
    "        return leaves\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(self.tree, sample) for sample in X])\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.8f}\")\n",
    "        print(f\"Precision: {precision:.8f}\")\n",
    "        print(f\"Recall: {recall:.8f}\")\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "dtl = ID3DecisionTree(max_depth=5)\n",
    "dtl.fit(x_train_set_processed_id3, y_train_set_id3.values)\n",
    "# Prediksi data validasi\n",
    "y_pred = dtl.predict(x_val_set_processed_id3)\n",
    "\n",
    "# Hitung akurasi\n",
    "accuracy = accuracy_score(y_val_set_id3, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Matriks kebingungan (confusion matrix)\n",
    "conf_matrix = confusion_matrix(y_val_set_id3, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Laporan klasifikasi\n",
    "class_report = classification_report(y_val_set_id3, y_pred, zero_division=0)\n",
    "print(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../model-id3.pkl', 'wb') as file:\n",
    "    pickle.dump(dtl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../model-id3.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_set = df.copy()\n",
    "x_training_set = training_set.drop('attack_cat', axis=1)\n",
    "y_training_set = training_set['attack_cat']\n",
    "\n",
    "x_test_set = df_test.copy()\n",
    "\n",
    "x_training_set_processed = pipe_id3.fit_transform(x_training_set, y_training_set)\n",
    "x_test_set_processed = pipe_id3.transform(x_test_set)\n",
    "\n",
    "loaded_model.fit(x_training_set_processed, y_training_set.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = loaded_model.predict(x_test_set_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 4 ... 5 2 2]\n",
      "['Normal' 'Normal' 'Fuzzers' ... 'Generic' 'DoS' 'DoS']\n"
     ]
    }
   ],
   "source": [
    "print(y_test_predict)\n",
    "reversed = le_attack_cat.inverse_transform(y_test_predict)\n",
    "print(reversed)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    \"id\": range(len(reversed)),\n",
    "    \"attack_cat\": reversed\n",
    "})\n",
    "\n",
    "result_df.head()\n",
    "result_df.to_csv(\"predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
