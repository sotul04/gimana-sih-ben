{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.454980</td>\n",
       "      <td>534.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.648037</td>\n",
       "      <td>8854.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>1.120856</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.264763</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id proto state       dur  sbytes  dbytes   sttl   dttl  sloss  dloss  ...  \\\n",
       "0   0   tcp   FIN  0.454980   534.0   268.0  254.0  252.0    2.0    1.0  ...   \n",
       "1   1   tcp   FIN  0.648037  8854.0   268.0  254.0  252.0    4.0    1.0  ...   \n",
       "2   2   tcp   FIN  1.120856  3440.0   642.0  254.0  252.0    5.0    3.0  ...   \n",
       "3   3   udp   INT  0.000001   244.0     0.0  254.0    NaN    0.0    0.0  ...   \n",
       "4   4   tcp   FIN  0.264763  1540.0  1644.0   31.0   29.0    4.0    4.0  ...   \n",
       "\n",
       "  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
       "0              0.0           0.0         0.0         5.0         5.0   \n",
       "1              0.0           NaN         0.0         6.0         6.0   \n",
       "2              0.0           0.0         0.0         4.0         4.0   \n",
       "3              0.0           0.0         0.0        10.0         4.0   \n",
       "4              NaN           0.0         0.0        13.0        11.0   \n",
       "\n",
       "   ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
       "0         2.0         2.0               2.0               1.0             2.0  \n",
       "1         1.0         1.0               1.0               1.0             5.0  \n",
       "2         1.0         2.0               1.0               1.0             4.0  \n",
       "3         2.0         4.0               2.0               1.0             4.0  \n",
       "4        10.0         7.0               6.0               1.0             7.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../dataset_train.csv')\n",
    "df = df.drop(columns=[\"label\"])\n",
    "df_test = pd.read_csv('../../dataset_test.csv')\n",
    "df.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['proto', 'state', 'service','is_sm_ips_ports','is_ftp_login','attack_cat']\n",
    "noncategorical_features = [col for col in df.columns.tolist() if col not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = df.copy()\n",
    "le_attack_cat = LabelEncoder()\n",
    "df['attack_cat'] = le_attack_cat.fit_transform(df['attack_cat'])\n",
    "\n",
    "train_set, val_set = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class FeatureImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='mean', fill_value=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputer for handling missing values.\n",
    "\n",
    "        :param strategy: The strategy to use for imputation ('mean', 'median', 'most_frequent', 'constant').\n",
    "                         Default is 'mean'.\n",
    "        :param fill_value: The value to use for the 'constant' strategy. Default is None.\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.imputer = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the imputer to the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        \"\"\"\n",
    "        self.imputer.fit(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data by imputing the missing values.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer and transform the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.fit_transform(X)\n",
    "\n",
    "    def get_imputation_statistics(self):\n",
    "        \"\"\"\n",
    "        Get the imputation statistics (e.g., mean or median values used for imputation).\n",
    "\n",
    "        :return: The statistics used for imputation (depending on the strategy)\n",
    "        \"\"\"\n",
    "        return self.imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        self.lower_bounds = np.percentile(X, self.lower_percentile * 100, axis=0)\n",
    "        self.upper_bounds = np.percentile(X, self.upper_percentile * 100, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        return np.clip(X, self.lower_bounds, self.upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method=\"standard\"):\n",
    "        self.method = method\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == \"standard\":\n",
    "            self.scaler = StandardScaler().fit(X)\n",
    "        elif self.method == \"minmax\":\n",
    "            self.scaler = MinMaxScaler().fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X) if self.scaler else X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDiscretizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features, bins=10, strategy='uniform'):\n",
    "        self.features = features\n",
    "        self.bins = bins\n",
    "        self.strategy = strategy\n",
    "        self.discretizers = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = pd.DataFrame(X, columns=self.features)\n",
    "\n",
    "        for feature in self.features:\n",
    "            if self.strategy == 'uniform':\n",
    "                discretizer = np.linspace(X_df[feature].min(), X_df[feature].max(), self.bins + 1)\n",
    "            elif self.strategy == 'quantile':\n",
    "                discretizer = np.quantile(X_df[feature], np.linspace(0, 1, self.bins + 1))\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid strategy: {self.strategy}\")\n",
    "\n",
    "            self.discretizers[feature] = discretizer\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = pd.DataFrame(X, columns=self.features)\n",
    "\n",
    "        for feature in self.features:\n",
    "            discretizer = self.discretizers[feature]\n",
    "            X_df[feature] = pd.cut(X_df[feature], bins=discretizer, labels=False, include_lowest=True, duplicates='drop')\n",
    "\n",
    "        return X_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "\n",
    "categorical_without_target = [x for x in categorical_features if x != 'attack_cat']\n",
    "onehot_features = ['service', 'proto']\n",
    "label_features = ['state']\n",
    "\n",
    "numeric_transformer_id3 = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='median')),\n",
    "    ('outlier_clipper', OutlierClipper(lower_percentile=0.01, upper_percentile=0.99)),\n",
    "    ('discretizer', FeatureDiscretizer(features=noncategorical_features, bins=10, strategy='uniform')),\n",
    "    ('scaler', FeatureScaling(method='standard'))\n",
    "])\n",
    "\n",
    "categorical_transformer_id3 = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "preprocessor_id3 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_id3, noncategorical_features),\n",
    "        ('cat', categorical_transformer_id3, categorical_without_target)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe_id3 = ImbPipeline([\n",
    "    ('preprocessor', preprocessor_id3),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_train_set_id3 = train_set.drop('attack_cat', axis=1)\n",
    "y_train_set_id3 = train_set['attack_cat']\n",
    "x_val_set_id3 = val_set.drop('attack_cat', axis=1)\n",
    "y_val_set_id3 = val_set['attack_cat']\n",
    "x_train_set_processed_id3 = pipe_id3.fit_transform(x_train_set_id3, y_train_set_id3)\n",
    "\n",
    "x_val_set_processed_id3 = pipe_id3.transform(x_val_set_id3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7354\n",
      "Confusion Matrix:\n",
      "[[  49   18   99  165   12    3   27   10    0    1]\n",
      " [  19   21  100  176   23    2    8   14    4    0]\n",
      " [ 112   98  775 1210   87   29   50   80   16    2]\n",
      " [ 150  143 1107 4330  298   77  196  303   18   14]\n",
      " [  19   40  144  431 1744   20  824  361   53    1]\n",
      " [   3    1   43   82   26 7803    7   10    0    0]\n",
      " [  31    5   57  196  923   17 9840  158   27    0]\n",
      " [   5   20  127  372  239    7   98 1190   10    2]\n",
      " [   0    3   13   23   68    1   34   81   36    0]\n",
      " [   0    0    1   13    2    0    1    8    1    2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.13      0.13       384\n",
      "           1       0.06      0.06      0.06       367\n",
      "           2       0.31      0.32      0.31      2459\n",
      "           3       0.62      0.65      0.64      6636\n",
      "           4       0.51      0.48      0.49      3637\n",
      "           5       0.98      0.98      0.98      7975\n",
      "           6       0.89      0.87      0.88     11254\n",
      "           7       0.54      0.57      0.56      2070\n",
      "           8       0.22      0.14      0.17       259\n",
      "           9       0.09      0.07      0.08        28\n",
      "\n",
      "    accuracy                           0.74     35069\n",
      "   macro avg       0.43      0.43      0.43     35069\n",
      "weighted avg       0.74      0.74      0.74     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Inisialisasi DecisionTreeClassifier dengan kriteria 'entropy'\n",
    "id3 = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "\n",
    "# Fit model ke data latih\n",
    "id3.fit(x_train_set_processed_id3, y_train_set_id3)\n",
    "\n",
    "# Prediksi data validasi\n",
    "y_pred = id3.predict(x_val_set_processed_id3)\n",
    "\n",
    "# Hitung akurasi\n",
    "accuracy = accuracy_score(y_val_set_id3, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Matriks kebingungan (confusion matrix)\n",
    "conf_matrix = confusion_matrix(y_val_set_id3, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Laporan klasifikasi\n",
    "class_report = classification_report(y_val_set_id3, y_pred, zero_division=0)\n",
    "print(f\"Classification Report:\\n{class_report}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7346\n",
      "Confusion Matrix:\n",
      "[[  27    7   72  257    6    1   10    4    0    0]\n",
      " [   6    5   65  226   28    1   23   13    0    0]\n",
      " [  21   35  431 1644  118   17  113   76    4    0]\n",
      " [  45   41  575 5101  340   23  360  146    5    0]\n",
      " [  10    8   57  384 1932   22  638  572   14    0]\n",
      " [   1    0    8  102   40 7764   17   43    0    0]\n",
      " [   3    0    0  269 1175   11 9381  412    3    0]\n",
      " [   4    6   62  527  296    2   75 1098    0    0]\n",
      " [   0    0    0    0   78    0   70   90   21    0]\n",
      " [   0    0    0   24    2    0    1    1    0    0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.07      0.11       384\n",
      "           1       0.05      0.01      0.02       367\n",
      "           2       0.34      0.18      0.23      2459\n",
      "           3       0.60      0.77      0.67      6636\n",
      "           4       0.48      0.53      0.50      3637\n",
      "           5       0.99      0.97      0.98      7975\n",
      "           6       0.88      0.83      0.86     11254\n",
      "           7       0.45      0.53      0.49      2070\n",
      "           8       0.45      0.08      0.14       259\n",
      "           9       0.00      0.00      0.00        28\n",
      "\n",
      "    accuracy                           0.73     35069\n",
      "   macro avg       0.45      0.40      0.40     35069\n",
      "weighted avg       0.73      0.73      0.72     35069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class ID3DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_gain=1e-4):\n",
    "        \"\"\"\n",
    "        Initialize the ID3 Decision Tree.\n",
    "\n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the tree (default: None, meaning no limit).\n",
    "        - min_samples_split: Minimum samples required to split a node.\n",
    "        - min_gain: Minimum information gain required for a split.\n",
    "        - use_gini: Use Gini Impurity instead of Entropy.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_gain = min_gain\n",
    "        self.tree = None\n",
    "\n",
    "    def entropy(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def information_gain(self, X_column, y):\n",
    "        parent_impurity = self.entropy(y)\n",
    "        values, counts = np.unique(X_column, return_counts=True)\n",
    "\n",
    "        weighted_impurity = np.sum(\n",
    "            [(counts[i] / len(X_column)) * self.entropy(y[X_column == value])\n",
    "             for i, value in enumerate(values)]\n",
    "        )\n",
    "        return parent_impurity - weighted_impurity\n",
    "\n",
    "    def best_split(self, X, y):\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            gain = self.information_gain(X[:, feature], y)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "\n",
    "        if best_gain < self.min_gain:\n",
    "            return None\n",
    "        return best_feature\n",
    "\n",
    "    def build_tree(self, X, y, depth=0):\n",
    "        if len(np.unique(y)) == 1:\n",
    "            return y[0]\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "        if len(y) < self.min_samples_split:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        feature = self.best_split(X, y)\n",
    "        if feature is None:\n",
    "            return Counter(y).most_common(1)[0][0]\n",
    "\n",
    "        tree = {feature: {}}\n",
    "        for value in np.unique(X[:, feature]):\n",
    "            sub_X = X[X[:, feature] == value]\n",
    "            sub_y = y[X[:, feature] == value]\n",
    "            subtree = self.build_tree(sub_X, sub_y, depth + 1)\n",
    "            tree[feature][value] = subtree\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict_sample(self, tree, sample):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "        feature = next(iter(tree))\n",
    "        value = sample[feature]\n",
    "        subtree = tree[feature].get(value)\n",
    "        if subtree is None:\n",
    "            return Counter(self.get_all_leaves(tree)).most_common(1)[0][0]\n",
    "        return self.predict_sample(subtree, sample)\n",
    "\n",
    "    def get_all_leaves(self, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return [tree]\n",
    "        leaves = []\n",
    "        for subtree in tree.values():\n",
    "            leaves.extend(self.get_all_leaves(subtree))\n",
    "        return leaves\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_sample(self.tree, sample) for sample in X])\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.8f}\")\n",
    "        print(f\"Precision: {precision:.8f}\")\n",
    "        print(f\"Recall: {recall:.8f}\")\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "dtl = ID3DecisionTree(max_depth=5)\n",
    "dtl.fit(x_train_set_processed_id3, y_train_set_id3.values)\n",
    "# Prediksi data validasi\n",
    "y_pred = dtl.predict(x_val_set_processed_id3)\n",
    "\n",
    "# Hitung akurasi\n",
    "accuracy = accuracy_score(y_val_set_id3, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Matriks kebingungan (confusion matrix)\n",
    "conf_matrix = confusion_matrix(y_val_set_id3, y_pred)\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Laporan klasifikasi\n",
    "class_report = classification_report(y_val_set_id3, y_pred, zero_division=0)\n",
    "print(f\"Classification Report:\\n{class_report}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../model-id3.pkl', 'wb') as file:\n",
    "    pickle.dump(dtl, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../model-id3.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_set = df.copy()\n",
    "x_training_set = training_set.drop('attack_cat', axis=1)\n",
    "y_training_set = training_set['attack_cat']\n",
    "\n",
    "x_test_set = df_test.copy()\n",
    "\n",
    "x_training_set_processed = pipe_id3.fit_transform(x_training_set, y_training_set)\n",
    "x_test_set_processed = pipe_id3.transform(x_test_set)\n",
    "\n",
    "loaded_model.fit(x_training_set_processed, y_training_set.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = loaded_model.predict(x_test_set_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 4 ... 5 2 2]\n",
      "['Normal' 'Normal' 'Fuzzers' ... 'Generic' 'DoS' 'DoS']\n"
     ]
    }
   ],
   "source": [
    "print(y_test_predict)\n",
    "reversed = le_attack_cat.inverse_transform(y_test_predict)\n",
    "print(reversed)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    \"id\": range(len(reversed)),\n",
    "    \"attack_cat\": reversed\n",
    "})\n",
    "\n",
    "result_df.head()\n",
    "result_df.to_csv(\"predictions.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
