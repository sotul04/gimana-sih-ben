{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder, RobustScaler,FunctionTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>proto</th>\n",
       "      <th>state</th>\n",
       "      <th>dur</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sloss</th>\n",
       "      <th>dloss</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_srv_src</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>ct_dst_ltm</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_src_dport_ltm</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.454980</td>\n",
       "      <td>534.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.648037</td>\n",
       "      <td>8854.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>1.120856</td>\n",
       "      <td>3440.0</td>\n",
       "      <td>642.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>udp</td>\n",
       "      <td>INT</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tcp</td>\n",
       "      <td>FIN</td>\n",
       "      <td>0.264763</td>\n",
       "      <td>1540.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id proto state       dur  sbytes  dbytes   sttl   dttl  sloss  dloss  ...  \\\n",
       "0   0   tcp   FIN  0.454980   534.0   268.0  254.0  252.0    2.0    1.0  ...   \n",
       "1   1   tcp   FIN  0.648037  8854.0   268.0  254.0  252.0    4.0    1.0  ...   \n",
       "2   2   tcp   FIN  1.120856  3440.0   642.0  254.0  252.0    5.0    3.0  ...   \n",
       "3   3   udp   INT  0.000001   244.0     0.0  254.0    NaN    0.0    0.0  ...   \n",
       "4   4   tcp   FIN  0.264763  1540.0  1644.0   31.0   29.0    4.0    4.0  ...   \n",
       "\n",
       "  ct_flw_http_mthd  is_ftp_login  ct_ftp_cmd  ct_srv_src  ct_srv_dst  \\\n",
       "0              0.0           0.0         0.0         5.0         5.0   \n",
       "1              0.0           NaN         0.0         6.0         6.0   \n",
       "2              0.0           0.0         0.0         4.0         4.0   \n",
       "3              0.0           0.0         0.0        10.0         4.0   \n",
       "4              NaN           0.0         0.0        13.0        11.0   \n",
       "\n",
       "   ct_dst_ltm  ct_src_ltm  ct_src_dport_ltm  ct_dst_sport_ltm  ct_dst_src_ltm  \n",
       "0         2.0         2.0               2.0               1.0             2.0  \n",
       "1         1.0         1.0               1.0               1.0             5.0  \n",
       "2         1.0         2.0               1.0               1.0             4.0  \n",
       "3         2.0         4.0               2.0               1.0             4.0  \n",
       "4        10.0         7.0               6.0               1.0             7.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../../dataset_train.csv')\n",
    "df = df.drop(columns=[\"label\"])\n",
    "df_test = pd.read_csv('../../dataset_test.csv')\n",
    "df.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['proto', 'state', 'service','is_sm_ips_ports','is_ftp_login','attack_cat']\n",
    "noncategorical_features = [col for col in df.columns.tolist() if col not in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = df.copy()\n",
    "le_attack_cat = LabelEncoder()\n",
    "df['attack_cat'] = le_attack_cat.fit_transform(df['attack_cat'])\n",
    "\n",
    "train_set, val_set = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy='mean', fill_value=None):\n",
    "        \"\"\"\n",
    "        Initialize the imputer for handling missing values.\n",
    "\n",
    "        :param strategy: The strategy to use for imputation ('mean', 'median', 'most_frequent', 'constant').\n",
    "                         Default is 'mean'.\n",
    "        :param fill_value: The value to use for the 'constant' strategy. Default is None.\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "        self.fill_value = fill_value\n",
    "        self.imputer = SimpleImputer(strategy=self.strategy, fill_value=self.fill_value)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the imputer to the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        \"\"\"\n",
    "        self.imputer.fit(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data by imputing the missing values.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the imputer and transform the data.\n",
    "\n",
    "        :param X: Features data with missing values\n",
    "        :return: Data with missing values imputed\n",
    "        \"\"\"\n",
    "        return self.imputer.fit_transform(X)\n",
    "\n",
    "    def get_imputation_statistics(self):\n",
    "        \"\"\"\n",
    "        Get the imputation statistics (e.g., mean or median values used for imputation).\n",
    "\n",
    "        :return: The statistics used for imputation (depending on the strategy)\n",
    "        \"\"\"\n",
    "        return self.imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierClipper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_percentile=0.01, upper_percentile=0.99):\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Ensure X is a DataFrame during fitting\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        self.lower_bounds = np.percentile(X, self.lower_percentile * 100, axis=0)\n",
    "        self.upper_bounds = np.percentile(X, self.upper_percentile * 100, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure X is a NumPy array during transformation\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "        return np.clip(X, self.lower_bounds, self.upper_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateRemover(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        self.fit(X,y)\n",
    "        X_unique, indices = np.unique(X[0], axis=0, return_index=True)\n",
    "        y_unique = X[1][indices]\n",
    "        return X_unique, y_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, k=10, score_func=f_classif):\n",
    "        \"\"\"\n",
    "        Initialize the feature selection process.\n",
    "\n",
    "        :param k: Number of top features to select. Default is 10.\n",
    "        :param score_func: Scoring function to evaluate the features. Default is f_classif (ANOVA F-test).\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.score_func = score_func\n",
    "        self.selector = SelectKBest(score_func=self.score_func, k=self.k)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the selector to the data.\n",
    "\n",
    "        :param X: Features\n",
    "        :param y: Target labels\n",
    "        \"\"\"\n",
    "        self.selector.fit(X, y)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Apply the feature selection transformation.\n",
    "\n",
    "        :param X: Features to transform\n",
    "        :return: Transformed features\n",
    "        \"\"\"\n",
    "        return self.selector.transform(X)\n",
    "\n",
    "    def fit_transform(self, X,y):\n",
    "        \"\"\"\n",
    "        Fit the selector and apply the transformation.\n",
    "\n",
    "        :param X: Features\n",
    "        :param y: Target labels\n",
    "        :return: Transformed features\n",
    "        \"\"\"\n",
    "        return self.selector.fit_transform(X, y)\n",
    "\n",
    "    def get_support(self):\n",
    "        \"\"\"\n",
    "        Get the mask of selected features.\n",
    "\n",
    "        :return: Mask of selected features (True/False)\n",
    "        \"\"\"\n",
    "        return self.selector.get_support()\n",
    "\n",
    "    def get_selected_features(self):\n",
    "        \"\"\"\n",
    "        Get the indices of the selected features.\n",
    "\n",
    "        :return: List of selected feature indices\n",
    "        \"\"\"\n",
    "        return self.selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureScaling(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method=\"standard\"):\n",
    "        self.method = method\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == \"standard\":\n",
    "            self.scaler = StandardScaler().fit(X)\n",
    "        elif self.method == \"minmax\":\n",
    "            self.scaler = MinMaxScaler().fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.scaler.transform(X) if self.scaler else X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedEncodingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, onehot_columns=None, label_columns=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - onehot_columns: List of column indices for one-hot encoding.\n",
    "        - label_columns: List of column indices for label encoding.\n",
    "        \"\"\"\n",
    "        self.onehot_columns = onehot_columns or []\n",
    "        self.label_columns = label_columns or []\n",
    "        self.onehot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore') if self.onehot_columns else None\n",
    "        self.label_encoder = LabelEncoder() if self.label_columns else None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformers to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        - y: Optional target labels, not used in this transformer.\n",
    "        \"\"\"\n",
    "        if self.onehot_columns:\n",
    "            # Fit one-hot encoder for the specified columns\n",
    "            self.onehot_encoder.fit(X[:, self.onehot_columns])\n",
    "\n",
    "        if self.label_columns:\n",
    "            # Fit label encoder for the specified columns\n",
    "            for col in self.label_columns:\n",
    "                self.label_encoder.fit(X[:, col])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the input data using the appropriate encoding methods.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        if self.onehot_columns:\n",
    "            onehot_encoded = self.onehot_encoder.transform(X[:, self.onehot_columns])\n",
    "            # Replace the original columns with one-hot encoded columns\n",
    "            X_transformed = np.delete(X_transformed, self.onehot_columns, axis=1)\n",
    "            X_transformed = np.hstack([X_transformed, onehot_encoded])\n",
    "\n",
    "        if self.label_columns:\n",
    "            for col in self.label_columns:\n",
    "                X_transformed[:, col] = self.label_encoder.transform(X[:, col])\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the transformers and transform the data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data array (2D).\n",
    "        - y: Optional target labels, not used in this transformer.\n",
    "        \"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEHandler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, random_state=None, sampling_strategy='auto'):\n",
    "        \"\"\"\n",
    "        Initialize the SMOTE handler.\n",
    "\n",
    "        :param random_state: Random state for reproducibility (default is None)\n",
    "        :param sampling_strategy: Defines the sampling strategy for SMOTE (default is 'auto')\n",
    "        \"\"\"\n",
    "        self.random_state = random_state\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.smote = SMOTE(random_state=self.random_state, sampling_strategy=self.sampling_strategy)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SMOTE model to the training data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.smote.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit and transform the dataset in one step.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector\n",
    "        :return: Balanced feature matrix X, and target vector y\n",
    "        \"\"\"\n",
    "        return self.smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "class DataNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, norm='l2'):\n",
    "        \"\"\"\n",
    "        Initialize the data normalizer.\n",
    "\n",
    "        :param norm: Norm to use for normalization, can be 'l1', 'l2', or 'max'. Default is 'l2'.\n",
    "        \"\"\"\n",
    "        self.norm = norm\n",
    "        self.normalizer = Normalizer(norm=self.norm)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the normalizer to the data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector (optional)\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.normalizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Normalize the data.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :return: Normalized feature matrix\n",
    "        \"\"\"\n",
    "        return self.normalizer.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform the data in one step.\n",
    "\n",
    "        :param X: Feature matrix\n",
    "        :param y: Target vector (optional)\n",
    "        :return: Normalized feature matrix\n",
    "        \"\"\"\n",
    "        return self.normalizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DimensionalityReducer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=None):\n",
    "        \"\"\"\n",
    "        Initialize the PCA dimensionality reducer.\n",
    "\n",
    "        :param n_components: Number of principal components to keep.\n",
    "                              If None, keeps all components.\n",
    "                              Can also be a float (explained variance ratio).\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.pca = PCA(n_components=self.n_components)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the PCA model to the data.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector (optional).\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.pca.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the data to the lower-dimensional space.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :return: Transformed data in lower-dimensional space.\n",
    "        \"\"\"\n",
    "        return self.pca.transform(X)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit and transform the data in one step.\n",
    "\n",
    "        :param X: Feature matrix.\n",
    "        :param y: Target vector (optional).\n",
    "        :return: Transformed data in lower-dimensional space.\n",
    "        \"\"\"\n",
    "        return self.pca.fit_transform(X[0])\n",
    "\n",
    "    def explained_variance_ratio(self):\n",
    "        \"\"\"\n",
    "        Return the explained variance ratio of each principal component.\n",
    "\n",
    "        :return: Array of explained variance ratios for each component.\n",
    "        \"\"\"\n",
    "        return self.pca.explained_variance_ratio_\n",
    "\n",
    "    def components(self):\n",
    "        \"\"\"\n",
    "        Return the principal components (eigenvectors).\n",
    "\n",
    "        :return: Matrix of principal components.\n",
    "        \"\"\"\n",
    "        return self.pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_without_target = [x for x in categorical_features if x != 'attack_cat']\n",
    "onehot_features = ['service', 'proto']\n",
    "label_features = ['state']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='median')),\n",
    "    ('outlier_clipper', OutlierClipper(lower_percentile=0.01, upper_percentile=0.99)),\n",
    "    ('scaler', FeatureScaling(method='standard'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', FeatureImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, noncategorical_features),\n",
    "        ('cat', categorical_transformer, categorical_without_target)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', FeatureSelection(k=10)),\n",
    "    ('smote', SMOTEHandler(random_state=42, sampling_strategy='auto')),\n",
    "    ('duplicate_remover', DuplicateRemover()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_priors = {}\n",
    "        self.mean = {}\n",
    "        self.var = {}  \n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Naive Bayes model on numerical data.\n",
    "        X: np.ndarray - Feature matrix\n",
    "        y: np.ndarray or pd.Series - Target labels\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(X, tuple):\n",
    "            X = X[0] \n",
    "        X = np.array(X)\n",
    "\n",
    "        self.classes = np.unique(y) \n",
    "        n_samples = len(y)\n",
    "\n",
    "        for cls in self.classes:\n",
    "            class_count = np.sum(y == cls)\n",
    "            self.class_priors[cls] = class_count / n_samples\n",
    "\n",
    "            X_c = X[np.array(y == cls)] \n",
    "\n",
    "            self.mean[cls] = X_c.mean(axis=0)\n",
    "            self.var[cls] = X_c.var(axis=0)\n",
    "\n",
    "    def _likelihood_num(self, class_idx, x):\n",
    "        \"\"\"\n",
    "        Compute likelihood for numerical features using Gaussian distribution.\n",
    "        class_idx: Class index\n",
    "        x: Feature vector\n",
    "        \"\"\"\n",
    "        mean = self.mean[class_idx]\n",
    "        var = self.var[class_idx]\n",
    "        likelihood = -((x - mean) ** 2) / (2 * var + 1e-6) \n",
    "        likelihood = np.exp(likelihood) / np.sqrt(2 * np.pi * var + 1e-6)\n",
    "        return likelihood.prod()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class for each sample in X.\n",
    "        X: np.ndarray - Feature matrix\n",
    "        \"\"\"\n",
    "        if isinstance(X, tuple):\n",
    "            X = X[0] \n",
    "        X = np.array(X)  \n",
    "\n",
    "        predictions = []\n",
    "        for x in X: \n",
    "            class_probs = {}\n",
    "            for cls in self.classes:\n",
    "                \n",
    "                class_prob = self.class_priors[cls]\n",
    "\n",
    "                class_prob *= self._likelihood_num(cls, x)\n",
    "\n",
    "                class_probs[cls] = class_prob\n",
    "\n",
    "            predictions.append(max(class_probs, key=class_probs.get))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate accuracy of the model.\n",
    "        y_true: np.ndarray - True labels\n",
    "        y_pred: np.ndarray - Predicted labels\n",
    "        \"\"\"\n",
    "        return np.mean(y_true == y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Naive Bayes Validation Accuracy: 0.3067\n",
      "Built-In Naive Bayes Validation Accuracy: 0.3049\n",
      "Accuracy Difference: 0.0018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "x_train_set = train_set.drop('attack_cat', axis=1)\n",
    "y_train_set = train_set['attack_cat']\n",
    "x_val_set = val_set.drop('attack_cat', axis=1)\n",
    "y_val_set = val_set['attack_cat']\n",
    "\n",
    "x_train_set_processed, y_train_set_processed = pipe.fit_transform(x_train_set, y_train_set)\n",
    "x_val_set_processed = pipe.transform(x_val_set)\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.fit(x_train_set_processed, y_train_set_processed)\n",
    "\n",
    "y_val_pred_custom = nb.predict(x_val_set_processed)\n",
    "\n",
    "accuracy_custom = nb.accuracy(y_val_set, y_val_pred_custom)\n",
    "print(f\"Custom Naive Bayes Validation Accuracy: {accuracy_custom:.4f}\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train_set_processed, y_train_set_processed)\n",
    "\n",
    "y_val_pred_builtin = gnb.predict(x_val_set_processed)\n",
    "\n",
    "accuracy_builtin = accuracy_score(y_val_set, y_val_pred_builtin)\n",
    "print(f\"Built-In Naive Bayes Validation Accuracy: {accuracy_builtin:.4f}\")\n",
    "\n",
    "print(f\"Accuracy Difference: {abs(accuracy_custom - accuracy_builtin):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../../model-nb.pkl', 'wb') as file:\n",
    "    pickle.dump(nb, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"../../model-nb.pkl\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to prediction.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x_test_set = df_test.copy()\n",
    "# Process the test set\n",
    "x_test_set_processed = pipe.transform(x_test_set)\n",
    "if isinstance(x_test_set_processed, tuple):\n",
    "    x_test_set_processed = x_test_set_processed[0]\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(x_test_set_processed)\n",
    "\n",
    "# Create a DataFrame for export\n",
    "# Assuming `x_test_set` has an identifier column like 'id' or use index\n",
    "y_test_predict = loaded_model.predict(x_test_set_processed)\n",
    "reversed = le_attack_cat.inverse_transform(y_test_predict)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    'id': range(len(reversed)),  # Replace 'id' with the appropriate identifier if available\n",
    "    'prediction': reversed\n",
    "})\n",
    "\n",
    "# Save predictions to CSV\n",
    "result_df.to_csv(\"../../submissions-nb.csv\", index=False)\n",
    "print(\"Predictions saved to prediction.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
